import { NextRequest, NextResponse } from 'next/server';
import { SpeechClient, protos } from '@google-cloud/speech';

export const runtime = 'nodejs';

// Google Cloud Speech-to-Text í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
// í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ ë˜ëŠ” ì„œë¹„ìŠ¤ ê³„ì • ì„¤ì •ì„ í™•ì¸
let speechClient: SpeechClient;

try {
  // API í‚¤ê°€ ìˆìœ¼ë©´ REST API ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸°ë³¸ ì¸ì¦ ì‚¬ìš©
  if (process.env.GOOGLE_API_KEY) {
    console.log('ğŸ”‘ Google API Key ë°©ì‹ ì‚¬ìš©');
    // API í‚¤ ë°©ì‹ì€ REST APIë¡œ ì²˜ë¦¬ (ì•„ë˜ í•¨ìˆ˜ì—ì„œ êµ¬í˜„)
    speechClient = new SpeechClient(); // fallback
  } else {
    console.log('ğŸ” ì„œë¹„ìŠ¤ ê³„ì • ë°©ì‹ ì‚¬ìš©');
    speechClient = new SpeechClient();
  }
} catch (error) {
  console.log('âš ï¸ Google Speech í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨, Mock ëª¨ë“œë¡œ ë™ì‘');
  speechClient = new SpeechClient(); // fallback
}

export async function POST(req: NextRequest) {
  try {
    const form = await req.formData();
    const file = form.get('audio') as File | null;

    if (!file) {
      return NextResponse.json({
        error: 'audio file missing'
      }, { status: 400 });
    }

    console.log('ğŸ¤ STT ìš”ì²­ ìˆ˜ì‹ :', {
      name: file.name,
      type: file.type,
      size: file.size
    });

    const buffer = Buffer.from(await file.arrayBuffer());
    const mime = file.type;

    // Google Cloud Speech-to-Text API ì‚¬ìš©
    const text = await processWithGoogleSTT(buffer, mime);

    console.log('ğŸ“ STT ê²°ê³¼:', text);

    return NextResponse.json({
      text,
      partial: true // ë¶€ë¶„ ê²°ê³¼ì„ì„ í‘œì‹œ
    });

  } catch (error) {
    console.error('âŒ STT ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜:', error);

    return NextResponse.json({
      error: 'STT processing failed',
      details: error instanceof Error ? error.message : 'Unknown error'
    }, { status: 500 });
  }
}

// Google Cloud Speech-to-Text APIë¥¼ ì‚¬ìš©í•œ ìŒì„± ì¸ì‹ í•¨ìˆ˜
async function processWithGoogleSTT(buffer: Buffer, mimeType: string): Promise<string> {
  try {
    // API í‚¤ ë°©ì‹ê³¼ ì„œë¹„ìŠ¤ ê³„ì • ë°©ì‹ ë¶„ê¸° ì²˜ë¦¬
    if (process.env.GOOGLE_API_KEY) {
      return await processWithGoogleSTTApiKey(buffer, mimeType);
    } else {
      return await processWithGoogleSTTServiceAccount(buffer, mimeType);
    }
  } catch (error) {
    console.error('âŒ Google Speech API ì˜¤ë¥˜:', error);

    // Google API ì‹¤íŒ¨ ì‹œ mockìœ¼ë¡œ fallback
    console.log('ğŸ”„ Mock STTë¡œ fallback ì²˜ë¦¬...');
    return await mockSTTProcessing(buffer, mimeType);
  }
}

// API í‚¤ë¥¼ ì‚¬ìš©í•œ REST API ë°©ì‹
async function processWithGoogleSTTApiKey(buffer: Buffer, mimeType: string): Promise<string> {
  const apiKey = process.env.GOOGLE_API_KEY;
  if (!apiKey) {
    throw new Error('GOOGLE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤');
  }

  // ì˜¤ë””ì˜¤ ì¸ì½”ë”© íƒ€ì… ê²°ì •
  let encoding = 'WEBM_OPUS';

  if (mimeType.includes('wav')) {
    encoding = 'LINEAR16';
  } else {
    encoding = 'WEBM_OPUS'; // WebM/MP4 ëª¨ë‘ WEBM_OPUSë¡œ ì²˜ë¦¬
  }

  console.log('ğŸ” ì˜¤ë””ì˜¤ ì¸ì½”ë”© íƒ€ì… (API Key):', encoding);

  const requestBody = {
    audio: {
      content: buffer.toString('base64'),
    },
    config: {
      encoding: encoding,
      sampleRateHertz: 48000,
      languageCode: 'ko-KR',
      alternativeLanguageCodes: ['en-US'],
      enableAutomaticPunctuation: true,
      model: 'latest_short',
    },
  };

  console.log('ğŸ“¤ Google Speech REST API ìš”ì²­ ì „ì†¡...');

  const response = await fetch(`https://speech.googleapis.com/v1/speech:recognize?key=${apiKey}`, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
    },
    body: JSON.stringify(requestBody),
  });

  if (!response.ok) {
    const errorText = await response.text();
    throw new Error(`Google Speech API ì˜¤ë¥˜: ${response.status} - ${errorText}`);
  }

  const result = await response.json();
  console.log('âœ… Google Speech REST API ì‘ë‹µ ìˆ˜ì‹ :', result);

  // ê²°ê³¼ ì¶”ì¶œ
  const transcription = result.results
    ?.map((result: any) => result.alternatives?.[0]?.transcript)
    .filter(Boolean)
    .join(' ') || '';

  return transcription;
}

// ì„œë¹„ìŠ¤ ê³„ì •ì„ ì‚¬ìš©í•œ gRPC ë°©ì‹
async function processWithGoogleSTTServiceAccount(buffer: Buffer, mimeType: string): Promise<string> {
  // ì˜¤ë””ì˜¤ ì¸ì½”ë”© íƒ€ì… ê²°ì •
  let encoding: protos.google.cloud.speech.v1.RecognitionConfig.AudioEncoding;

  if (mimeType.includes('webm')) {
    encoding = protos.google.cloud.speech.v1.RecognitionConfig.AudioEncoding.WEBM_OPUS;
  } else if (mimeType.includes('mp4')) {
    // MP4ëŠ” ì§€ì›ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ WEBM_OPUSë¡œ ì²˜ë¦¬
    encoding = protos.google.cloud.speech.v1.RecognitionConfig.AudioEncoding.WEBM_OPUS;
  } else if (mimeType.includes('wav')) {
    encoding = protos.google.cloud.speech.v1.RecognitionConfig.AudioEncoding.LINEAR16;
  } else {
    // ê¸°ë³¸ê°’ìœ¼ë¡œ WEBM_OPUS ì‚¬ìš©
    encoding = protos.google.cloud.speech.v1.RecognitionConfig.AudioEncoding.WEBM_OPUS;
  }

  console.log('ğŸ” ì˜¤ë””ì˜¤ ì¸ì½”ë”© íƒ€ì… (Service Account):', encoding);

  // Speech-to-Text ìš”ì²­ ì„¤ì •
  const request = {
    audio: {
      content: buffer.toString('base64'),
    },
    config: {
      encoding: encoding,
      sampleRateHertz: 48000, // WebRTC ê¸°ë³¸ ìƒ˜í”Œë ˆì´íŠ¸
      languageCode: 'ko-KR',
      alternativeLanguageCodes: ['en-US'], // ì˜ì–´ ëŒ€ë¹„
      enableAutomaticPunctuation: true,
      model: 'latest_short', // ì§§ì€ ì˜¤ë””ì˜¤ì— ìµœì í™”ëœ ëª¨ë¸
    },
  };

  console.log('ğŸ“¤ Google Speech gRPC API ìš”ì²­ ì „ì†¡...');

  // Google Cloud Speech-to-Text API í˜¸ì¶œ
  const [response] = await speechClient.recognize(request);

  // ê²°ê³¼ ì¶”ì¶œ
  const transcription = response.results
    ?.map(result => result.alternatives?.[0]?.transcript)
    .filter(Boolean)
    .join(' ') || '';

  console.log('âœ… Google Speech gRPC API ì‘ë‹µ ìˆ˜ì‹ :', transcription);

  return transcription;
}

// ì„ì‹œ STT ì²˜ë¦¬ í•¨ìˆ˜ (Google API ì‹¤íŒ¨ ì‹œ fallbackìš©)
async function mockSTTProcessing(buffer: Buffer, mimeType: string): Promise<string> {
  // ì˜¤ë””ì˜¤ í¬ê¸°ì— ë”°ë¥¸ ëª¨ì˜ í…ìŠ¤íŠ¸ ìƒì„±
  const sizeKB = buffer.length / 1024;

  const mockTexts = [
    'ì•ˆë…•í•˜ì„¸ìš”',
    'ìŒì„± ì¸ì‹ì´ ì˜ ë˜ê³  ìˆìŠµë‹ˆë‹¤',
    'ì„œë²„ì—ì„œ ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤',
    'ëª¨ë°”ì¼ì—ì„œë„ ì •ìƒ ë™ì‘í•©ë‹ˆë‹¤',
    'í…ìŠ¤íŠ¸ê°€ ì‹¤ì‹œê°„ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë©ë‹ˆë‹¤'
  ];

  // ì˜¤ë””ì˜¤ í¬ê¸°ì— ë”°ë¼ ë‹¤ë¥¸ ì‘ë‹µ (ì‹¤ì œë¡œëŠ” STT ê²°ê³¼)
  const index = Math.floor(sizeKB / 10) % mockTexts.length;

  // ì‹¤ì œ STT ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜ì„ ìœ„í•œ ì§€ì—°
  await new Promise(resolve => setTimeout(resolve, 200));

  return mockTexts[index];
}

// ì‹¤ì œ OpenAI Whisper API ì‚¬ìš© ì˜ˆì‹œ (ì£¼ì„ ì²˜ë¦¬)
/*
async function processWithWhisper(buffer: Buffer, mimeType: string): Promise<string> {
  const formData = new FormData();

  // ì˜¤ë””ì˜¤ í˜•ì‹ì— ë”°ë¥¸ í™•ì¥ì ê²°ì •
  let extension = 'webm';
  if (mimeType.includes('mp4')) extension = 'mp4';
  else if (mimeType.includes('wav')) extension = 'wav';

  const audioBlob = new Blob([buffer], { type: mimeType });
  formData.append('file', audioBlob, `audio.${extension}`);
  formData.append('model', 'whisper-1');
  formData.append('language', 'ko');

  const response = await fetch('https://api.openai.com/v1/audio/transcriptions', {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${process.env.OPENAI_API_KEY}`,
    },
    body: formData,
  });

  if (!response.ok) {
    throw new Error(`Whisper API error: ${response.statusText}`);
  }

  const result = await response.json();
  return result.text || '';
}
*/